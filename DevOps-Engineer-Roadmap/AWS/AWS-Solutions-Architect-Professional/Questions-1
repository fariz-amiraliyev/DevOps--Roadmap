AWS Certified Solutions Architect - Professional 2020


1. You are consulting for a large multi-national company that is designing their
   AWS account structure. The company policy says that they must maintain a centralized
   logging repository but localized security management. For economic efficiency,
   they also require all sub-account charges to roll up under one invoice.
   Which of the following solutions most efficiently addresses these requirements?

   Service Control Policies Create a stand-alone consolidated logging account and configure all sub-account
   CloudWatch and CloudTrail activity to route to that account. Use an SCP to restrict sub-accounts
   from changing CloudWatch and CloudTrail configuration. Configure consolidated billing under a
   single account and register all sub-accounts to that billing account.
   Create localized IAM Admin accounts for each sub-account.

   Service Control Policies are an effective way to broadly restrict access to certain
   features of sub-accounts. Use of a single separate logging account is an
   effective way to create a secure logging repository.


2. A food service business has begun an initiative to migrate all applications and data to the AWS cloud.
   Governance needs to be established before any migrations can occur. Business units such as sales, marketing,
   and product management have fluctuating infrastructure capacity and security requirements, while other
   business units like finance, operations, and human resources have more static demand. Security policies
   and compliance needs vary by project group within each business units. Each business unit is responsible
   for it's own cost center, and the finance group would like cost reporting to be as streamlined as possible.
   Which AWS account structure will best satisfy the company's governance needs?

   Use AWS Organizations to create a core Organizational Unit that contains a billing account, a shared services account,
   and a log archive account. Create an Organizational Unit for each business unit that contains accounts for each project
   group within the business unit. Establish standard tags to sort the AWS Detailed Billing report by cost center.


   Leveraging AWS Organizations to manage an account structure with a core Organizational Unit and Organizational Units
   for each business unit provides flexibility for future organizational changes. Creating an account for each project group
   facilitates security policy differences within business units, and limits the exposure of a single security event.
   Managing differing security requirements by project group in a single account will require more governance maintenance.
   Creating billing, shared services, and log archive accounts in multiple Organizational Units will result in duplication
   of services, and can be done at the core level. Laying the Foundation: Setting Up Your Environment for Cost Optimization
   AWS Landing Zone


   3. AWS Cost Management encompasses a number of services to help you to organize, control and optimize your
     AWS costs and usage. Which of the following Cost Management related tools gives you the ability to set alerts when costs or usage are exceeded?
     AWS Budgets
     The correct answer is AWS Budgets. AWS Cost Explorer lets you visualize, understand, and manage your AWS costs and usage over time.
     AWS Cost & Usage Report lists AWS usage for each service category used by an account and its IAM users and finally, Reserved
     Instance Reporting provides a number of RI-specific cost management solutions to help you better understand and manage
     RI Utilization and Coverage. AWS Budgets

  4. You have been entrusted to act as the interim AWS Administrator following the departure of the erstwhile Administrator
     in your company. You notice that there are several existing roles called role-engineer, role-manager, role-qa, role-dba,
     role-data-scientist, etc. When a new person joins the company, the new IAM user simply assumes the right role while using
     AWS - this allows central management of permissions and eliminates the need to manage permissions on a per-user basis.
     A new QA hire joins the company a few days later. You create an IAM User for her. You attach a Policy to the new IAM User
     that allows Action STS AssumeRole on any Resource. However, when this employee logs in the same day and tries to switch roles
     to role-qa, she is denied and is unable to assume the role-qa Role. What could be one reason why this is happening and how can
     it be best fixed?


     You have not modified the Trust Policy of the IAM Role role-qa to allow the new IAM User to assume the Role.
     To fix this, add the arn of the new IAM User to the Principal element of the Trust Policy of the Role

     Why is this correct?
In order to allow an IAM User to successfully assume an IAM Role, two things must happen. First, the Policy attached to the User
must allow the action STS AssumeRole. This is already true according to the question. Second, the Trust Policy of the Role itself
must allow the User in question to assume the Role. This second condition can be met if we specify the arn of the User in the
Principal element of the Trust Policy. In general, this question can be answered if the candidate is familiar with the concept
of Principal in a Role, see link - A Principal within an Amazon IAM Role specifies the user (IAM user, federated user, or
 assumed-role user),
AWS account, AWS service, or other principal entity that is allowed or denied to assume or impersonate that Role.
Trust Policy is different
than the Policy permissions - think of Policy Permissions as [what can be accessed] and Trust Policy as [who can access].
Trust Policy cannot belong to an IAM User, hence the choice that claims the problem to be an unmodified User Trust Policy is incorrect.
IAM changes are instantly effective, so the choice that points at the need of a time delay is also incorrect. Among the other two choices,
the knowledge needed to pick the right one is an awareness of the Principal element. AWS JSON Policy Elements - Principal IAM Roles


5. A company owns multiple AWS accounts managed in an AWS Organization. You need to generate daily cost and usage reports
   that include the activities of all the member accounts. The reports should track the AWS usage for each resource type and
   provide estimated charges. The report files also need to be delivered to an Amazon S3 bucket for storage. How would you create
   the required reports?

   In the master account of the AWS Organization, generate the AWS Cost and Usage Reports and save the reports in an
   S3 bucket. Modify the bucket policy to allow the billing reports service to put objects.

   Why is this correct?
The consolidated billing feature in AWS Organization does not generate billing reports automatically.
You need to configure the AWS Cost and Usage Reports in the master account and use an S3 bucket to store the reports.
The generated reports include activities for all the member accounts and it is not required to create a report
in each member's account. The option of CloudWatch Event rule and Lambda function may work however it
is not a straightforward solution. AWS Cost and Usage Report



6. You are helping a client troubleshoot a problem. The client has several Ubuntu Linux servers in a private subnet within a VPC.
The servers are configured to use IPv6 only and must periodically communicate to the Internet to get security patches for
applications installed on them. Unfortunately, the servers are unable to reach the internet. An internet gateway has been
deployed in the public subnet in the VPC and default routes are configured. Which of the following could fix the issue?

Implement an Egress-only Gateway in the public subnet and configure an IPv6 default route for the private subnet to the gateway.
With IPv6 you only requires an Egress-Only Internet Gateway and an IPv6 route to reach the internet from within a VPC. Configure Private IPv6 Subnets


7. You are helping a client design their AWS network for the first time. They have a fleet of servers that run a
very precise and proprietary data analysis program. It is highly dependent on keeping the system time across the servers in sync.
As a result, the company has invested in a high-precision stratum-0 atomic clock and network appliance which all servers sync to using NTP.
They would like any new AWS-based EC2 instances to also be in sync as close as possible to the on-prem atomic clock as well.
What is the most cost-effective, lowest maintenance way to design for this requirement?


Configure a DHCP Option Set with the on-prem NTP server address and assign it to each VPC. Ensure NTP (UDP port 123) is allowed between AWS and your on-prem network.


8. For large organizationally complex AWS landscapes, it is considered a best practice to combine a tagging strategy with lifecycle tracking of
various projects to identify orphaned resources that are no longer generating value for the organization and should be decommissioned.
With which AWS Well-Architected Framework Pillar is this best practice most aligned?

Tagging has many uses but one strong use-case is in being able to tie resources that incur costs with cost centers or projects to create a direct
line of sight to actual AWS expenses. If this visibility does not exist, costs tend to increase because "someone else is paying." A Best Practice
of the Cost Optimization Pillar is to maintain expenditure awareness. AWS Well-Architected - Build secure, efficient, cloud enabled applications


9. A client has asked you to help troubleshoot a Service Control Policy. Upon reviewing the policy,
you notice that they have used multiple "Statement" elements for each Effect/Action/Resource object but
the policy is not working. What would you suggest next?

Change the policy to combine the multiple Statement elements into one element with an object array.
The syntax for an SCP requires only one Statement element. You can have multiple objects within a single Statement element though.
Troubleshooting AWS Organizations Policies - AWS Organizations


10. You work for a genetics company that has extremely large datasets stored in S3. You need to minimize storage costs,
while maintaining mandated restore times that depend on the age of the data. Data 30-59 days old must be available immediately,
and data ≥ 60 days old must be available within 12 hours. Which of the following options below should you consider?

You should use S3 - IA for the data that needs to be accessed immediately, and you should use Glacier for the data that must be
recovered within 12 hours. S3 - RRS and 1Zone-IA would not be suitable solution for irreplaceable data or data that required
immediate access (reduced Durability or Availability), and CloudFront is a CDN service, not a storage solution. The use of
absolute words like 'Must' is an important clue as it will eliminate options where the case may not be possible such as with
OneZone-IA. S3 - Infrequent Access About Glacier


11. You are helping a client troubleshoot a new Direct Connect connection. The connection is up and you can ping the AWS peer
IP address, but the BGP peering session cannot be established. What should be your next logical troubleshooting steps?

Make sure no firewalls or ACLs are blocking TCP port 179 or any high-numbered ephemeral ports.
Ensure that the local ASNs and AWS-side ASNs are properly configured.

Because the connection is up and we can ping the AWS peer, the problem must be at a higher level on the OSI model than the
Physical or Data layers. BGP uses TCP port 179 to communicate routes so we should check that no NACL or SG is blocking it.
Additionally, we should make sure the ASNs are properly configured in the proper ranges. Troubleshooting
AWS Direct Connect - AWS Direct Connect


  12. Your client is a software company starting their initial architecture steps for their new multi-tenant
      CRM application. They are concerned about responsiveness for companies with employees scattered around the globe.
      Which of the following ideas should you suggest to help with the overall latency of the application?

      Architect the system to use as many static objects as possible with high TTL. Use CloudFront to retrieve both
      static and dynamic objects. POST and PUT new data through CloudFront.

      Install key parts of the application in multiple AWS regions chosen to balance latency for geographically diverse users.
      Use Lambda@Edge to dynamically select the appropriate region based on the users location.

      CloudFront can cache both static and dynamic content. By setting a high TTL, we allow CloudFront to serve content
      longer before having to refresh
      from the origin. Additionally, Lambda@Edge can intercept the request and direct the requester to a region based on
      the geographic origin of the request.
      Lambda@Edge Now Supports Content-Based Dynamic Origin Selection, Network Calls from Viewer Events, and Advanced
      Response Generation



13.  You've begun deploying EC2 and VMware Cloud on AWS instances to host various applications which you'd like to make accessible
to those who authenticate via an on-premises Active Directory domain. You've configured AWS Managed Microsoft
AD in the same region as the EC2 and VMware Cloud on AWS instances with a one-way trust back to the corporate AD domain.
You're able to seamlessly join new EC2 Windows instances to the Managed AD domain at launch, but the EC2 Linux and
VMware Cloud on AWS instances don't show up in the domain when launched. What additional actions need to take place
in order to seamlessly join all the instances to the domain at launch?
Create a bootstrap script to install a Kerberos client package and perform a Realm Join command for the EC2 Linux instances, and
add a VMware Cloud NSX Compute Gateway (CGW) Firewall Rule for the VMware Cloud on AWS instances


14. You are a developer for a Aerospace company. As part of an outreach and education program, the company has
financed the construction of a free public service that provides weather forecasts for the sun. Anyone can make a
call to this REST service and receive up-to-date information on forecasted sun flare or sun spots that might have
an electromagnetic impact here on Earth. You are in the final stages of developing this new serverless application
based on DynamoDB, Lambda and API Gateway. During performance testing, you notice inconsistent response times for
the service. You had expected the API to be relatively consistent since its just retrieving data from DynamoDB and
returning it as JSON via the API Gateway. What might account for this variation in response time?

Inconsistent response times can have a few different causes. The exact nature of the testing is not explained but we
can anticipate a few causes. If you have enabled API Gateway caching, the gateway can return a result from its cache
without having to go back to a supplying service or database. This can result in various response rates depending on
if an item is in the cache or not. (The question did not specify we had slow response...just inconsistent response
which could be a response faster than we expected.) When a Lambda function is run for the first time or after an update,
AWS must provision the Lambda environment and pull in any external dependencies. This can result in a slower response time
at first but faster later. Also, if we do not have sufficient RCU for our DynamoDB table, we could run into throttling of
the reads which could appear as inconsistent response times. AWS Lambda Execution Context - AWS Lambda Enable API Caching
to Enhance Responsiveness - Amazon API Gateway



15. You have configured your VPC CIDR as 10.0.0.128/25. What IP address would you expect is assigned to the DNS server?
The DNS is a reserved address in the VPC CIDR and will be the second usable address. In this example, .128 is the network address,
.129 is reserved for the router, .130 is reserved for the DNS. VPCs and Subnets - Amazon Virtual Private Cloud

16. You currently are using several CloudFormation templates. They are used to create stacks that include the resources
of VPC subnets, Elastic Load Balancers, Auto Scaling groups, etc. You want to deploy all the stacks with a root stack so
that all the resources can be configured at one time. Meanwhile, you need to isolate information sharing to within this stack group,
which means other stacks outside of the stack group can not import its resources. For example, one stack creates a VPC subnet resource
and this subnet can only be referenced by the stack group. What is the best way to implement this?


Create nested stacks with the "AWS::CloudFormation::Stack" resources. Use the outputs from one stack in the nested stack group as
inputs to another stack in the group if needed.

16.


19. You are working with a customer to implement some better security policies. They have a group of
   remote employees working on a confidential project that uses some proprietary Windows software and stores
   data in S3. The Chief Information Security Officer is concerned about the threat of the desktop software or
   confidential data being smuggled out to a competitor. What architecture would you recommend to best address this concern?

   Provision Amazon Workspaces in a secured private VPC. Do not enable Internet access for the Workspaces.
   Create a VPC Gateway Endpoint to S3 and implement an endpoint policy that explicitly allows access to the required bucket.
   Assign an S3 bucket policy that denies access unless the sourceVpce matches the VPC endpoint. Supply the users with instructions
   on downloading and login into the Workspaces instances.


   Using a locked down virtual desktop concept would be the best way to manage this. AWS WorkSpaces provides
   this complete with client software to log into the desktops. These Workspaces can be walled off from the Internet.
   Using policies, you could allow access from only those in the Workspaces VPC. What Is Amazon WorkSpaces? -
   Amazon WorkSpaces Endpoints for Amazon S3 - Amazon Virtual Private Cloud


20.
You have been contracted by a Security Company to build a face recognition service for its customer,
Department of Corrections, in the AWS Cloud. Whenever a new inmate or personnel joins a facility,
their facial image will be taken by an application running on a laptop, and stored centrally, along
with metadata like their name. They will have a second application getting a live image feed from
cameras installed throughout the secure areas of the facility. Whenever the second application receives
an image from a camera, it needs to check against the pool of images stored centrally to check if there
is a match. If a match is found, the location must be saved along with the timestamp and name in a database
 which can later be used to query the location of a person at or near a given time-period. How will you,
 as the AWS Architect, design this suite of applications?


 Store the images taken at the time of joining in an S3 bucket, along with the metadata. Configure a Lambda
 function to be triggered on putObject event of the bucket. Invoke Amazon Rekognition from the Lambda
 function to index the face in the image as a Collection, returning a Face id. Store this face id and Name
 of the person in a Dynamodb database. Later, when a match is required, query the Amazon Rekognition
 Collection with the image taken by security cameras. Rekognition will return a set of face ids that
 have potentially matched. If a face id is found whose confidence value is higher than a predefined set
 value, query the Dynamodb database for the name belonging to the face id. Then write a record containing
  the name, timestamp and the location id of the camera to an RDS database for later querying

  This question tests the knowledge of the various Machine Learning technologies and services offered by AWS.
  Amazon Rekognition is the AWS AI service for image and video analysis. The question also offers AWS Sagemaker,
  AWS Personalize and AWS Comprehend as alternatives. AWS Sagemaker is used to build and train Machine Learning models.
  Hence, it is not relevant in this scenario, as the use case is not about training a model. Amazon Personalize is an ML
   service that enables developers to create individualized recommendations for customers using their applications.
   This use case is not related to recommendations, hence we can eliminate AWS Personalize. Amazon Comprehend is a natural
   language processing (NLP) service that uses ML to find relationships and insights in text. We can eliminate Comprehend
   as this is an image analysis scenario, as opposed to text analysis. Build Your Own Face Recognition Service Using Amazon
   Rekognition Various components of Machine Learning on AWS

21. You have been asked to design a landscape that can facilitate the upload very high resolution photos
    from mobile devices, gather metadata on objects in the photos and store that metadata for analysis.
    Which of the following components would you use for this use-case for quickest implementation and best scalability?

    Rekognition, DynamoDB, S3
    DynamoDB and S3 represent the most reasonable and scalable choices in this list for metadata storage (DynamoDB) and
    file upload (S3). Kinesis has size limits on the inbound object so it would not be appropriate for use cases that
    involve potentially large files like photos. Amazon Rekognition is image processing service that can extract
    metadata on objects in a photograph. Amazon Rekognition – Video and Image - AWS

22. You need to design a new CloudFormation template for several security groups. The security groups are required in
    different environments such as QA, Dev and Production. The allowed CIDR range in the security group ingress rule
    depends on environments. For example, the allowed inbound address range is 10.0.0.0/16 for non-production and
    10.1.0.0/16 for production. You prefer to maintain a single template for all the environments. What is the best
    method for you to choose?

    Use the environment type as an input parameter and create a condition based on the parameter.
    In the AWS::EC2::SecurityGroupIngress resource, use Fn::If to check the condition and return related source CIDR range.

    CloudFormation has an optional Conditions section that contains statements to determine whether or not entities should be
    created or configured. Then you can use the "Fn::If" function to check the condition and return different values.
    You do not need to use Jenkins to pre-process the template and there are no "Fn::Case" and "Fn::Switch" intrinsic
    functions. CloudFormation conditions

23.
