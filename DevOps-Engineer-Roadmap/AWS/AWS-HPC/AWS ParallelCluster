AWS ParallelCluster is an AWS supported open source cluster management tool that helps you to deploy and manage
High Performance Computing (HPC) clusters in the AWS Cloud.

Built on the open source CfnCluster project, AWS ParallelCluster enables you to
quickly build an HPC compute environment in AWS. It automatically sets up the
required compute resources and shared file system. You can use AWS ParallelCluster
with a variety of batch schedulers, such as AWS Batch, SGE, Torque, and Slurm. AWS
ParallelCluster facilitates quick start proof of concept deployments and production deployments.
You can also build higher level workflows, such as a genomics portal that automates an entire
DNA sequencing workflow, on top of AWS ParallelCluster.


1. INSTALL AWS PARALLELCLUSTER
   pip-3.6 install aws-parallelcluster -U --user

   CONFIGURE PARALLELCLUSTER:

   # generate a new key-pair
aws ec2 create-key-pair --key-name lab-3-your-key --query KeyMaterial --output text > ~/.ssh/lab-3-key
chmod 600 ~/.ssh/lab-3-key

IFACE=$(curl --silent http://169.254.169.254/latest/meta-data/network/interfaces/macs/)
SUBNET_ID=$(curl --silent http://169.254.169.254/latest/meta-data/network/interfaces/macs/${IFACE}/subnet-id)
VPC_ID=$(curl --silent http://169.254.169.254/latest/meta-data/network/interfaces/macs/${IFACE}/vpc-id)
REGION=$(curl --silent http://169.254.169.254/latest/meta-data/placement/availability-zone | sed 's/[a-z]$//')

mkdir -p ~/.parallelcluster
cat > ~/.parallelcluster/config << EOF
[aws]
aws_region_name = ${REGION}

[cluster default]
key_name = lab-3-your-key
vpc_settings = public
base_os = alinux2
scheduler = slurm

[vpc public]
vpc_id = ${VPC_ID}
master_subnet_id = ${SUBNET_ID}

[global]
cluster_template = default
update_check = false
sanity_check = true

[aliases]
ssh = ssh {CFN_USER}@{MASTER_IP} {ARGS}
EOF

cat ~/.parallelcluster/config


C. CREATE A CLUSTER CONFIG
Now that you installed AWS ParallelCluster and created a default configuration, you can create a configuration file to build a simple HPC system. This file is generated in your home directory.

Generate the cluster with the following settings:

Head-node and compute nodes: c4.xlarge instances. You can change the instance type if you like, but you may run into EC2 limits that may prevent you from creating instances or create too many instances.
We use a placement group to maximize the bandwidth between instances and reduce the latency. This packs instances close together inside an Availability Zone.
The cluster has 0 compute nodes when starting and maximum size set to 8 instances. We are using Auto Scaling Groups that will grow and shrink between the min and max limits based on the cluster utilization and job queue backlog.
A GP2 Amazon EBS volume will be attached to the head-node then shared through NFS to be mounted by the compute nodes on /shared. It is generally a good location to store applications or scripts. Keep in mind that the /home directory is shared on NFS as well.
SLURM will be used as a job scheduler but there are other options you may consider in the future such as SGE.
We disable Intel Hyper-threading by setting disable_hyperthreading = true in the configuration file.


IFACE=$(curl --silent http://169.254.169.254/latest/meta-data/network/interfaces/macs/)
SUBNET_ID=$(curl --silent http://169.254.169.254/latest/meta-data/network/interfaces/macs/${IFACE}/subnet-id)
VPC_ID=$(curl --silent http://169.254.169.254/latest/meta-data/network/interfaces/macs/${IFACE}/vpc-id)
REGION=$(curl --silent http://169.254.169.254/latest/meta-data/placement/availability-zone | sed 's/[a-z]$//')

cd ~/environment
cat > my-cluster-config.ini << EOF
[aws]
aws_region_name = ${REGION}

[global]
cluster_template = default
update_check = false
sanity_check = true

[cluster default]
key_name = lab-3-your-key
base_os = alinux2
vpc_settings = public
ebs_settings = myebs
compute_instance_type = c4.xlarge
master_instance_type = c4.xlarge
cluster_type = ondemand
placement_group = DYNAMIC
placement = compute
initial_queue_size = 0
max_queue_size = 8
disable_hyperthreading = true
s3_read_write_resource = *
scheduler = slurm

[vpc public]
vpc_id = ${VPC_ID}
master_subnet_id = ${SUBNET_ID}

[ebs myebs]
shared_dir = /shared
volume_type = gp2
volume_size = 20

[aliases]
ssh = ssh {CFN_USER}@{MASTER_IP} {ARGS}
EOF


BUILD AN HPC CLUSTER:
pcluster create hpclab-yourname -c my-cluster-config.ini

pcluster list --color

pcluster ssh hpclab-yourname -i ~/.ssh/lab-3-key

SLURM from SchedMD is one of the batch schedulers that you can use in AWS ParallelCluster. For an overview of the SLURM commands, see the SLURM Quick Start User Guide.



List existing partitions and nodes per partition. You should see two nodes if your run this command after creating your cluster, and zero nodes if running it 10 minutes after creation (default cooldown period for AWS ParallelCluster, you don’t pay for what you don’t use).

sinfo
List jobs in the queues or running. Obviously, there won’t be any since we did not submit anything…yet!

squeue
Module Environment
Lmod is a fairly standard tool used to dynamically change your environment.

List available modules

module av
Load a particular module. In this case, this command loads IntelMPI in your environment and checks the path of mpirun.

module load intelmpi
which mpirun
NFS Shares
List mounted volumes. A few volumes are shared by the head-node and will be mounted on compute instances when they boot up. Both /shared and /home are accessible by all nodes.

showmount -e localhost
