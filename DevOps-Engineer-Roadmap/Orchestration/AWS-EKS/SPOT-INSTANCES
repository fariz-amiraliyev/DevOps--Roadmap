if we need new worker nodes on running spot instances:

ADD EC2 WORKERS - SPOT
But first, we will add a new label to the OnDemand worker nodes:
kubectl label nodes --all 'lifecycle=OnDemand'

Create Spot worker nodes:
cat << EoF > ~/environment/eks-workshop-ng-spot.yaml
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: eksworkshop-eksctl
  region: ${AWS_REGION}
nodeGroups:
  - name: ng-spot
    labels:
      lifecycle: Ec2Spot
    taints:
      spotInstance: true:PreferNoSchedule
    minSize: 2
    maxSize: 5
    instancesDistribution:
      instanceTypes:
        - m5.large
        - m4.large
        - m5d.large
        - m5a.large
        - m5ad.large
        - m5n.large
        - m5dn.large
      onDemandBaseCapacity: 0
      onDemandPercentageAboveBaseCapacity: 0 # all the instances will be Spot Instances
      spotAllocationStrategy: capacity-optimized # launch Spot Instances from the most availably Spot Instance pools
EoF

eksctl create nodegroup -f ~/environment/eks-workshop-ng-spot.yaml

During the creation of the Node Group, we have configured a node-label so
that kubernetes knows what type of nodes we have provisioned. We set the
lifecycle for the nodes as Ec2Spot.

We are also tainting with PreferNoSchedule to prefer pods not be scheduled on
Spot Instances. This is a “preference” or “soft” version of NoSchedule – the system
will try to avoid placing a pod that does not tolerate the taint on the node,
but it is not required.

kubectl get nodes --sort-by=.metadata.creationTimestamp
kubectl get nodes --label-columns=lifecycle --selector=lifecycle=Ec2Spot
kubectl get nodes --label-columns=lifecycle --selector=lifecycle=OnDemand
