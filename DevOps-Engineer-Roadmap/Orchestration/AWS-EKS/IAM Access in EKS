IAM is the de-facto method of authorization in AWS.

Step 1: Create IAM Roles

The first step to using kube2iam is to create IAM roles for your pods.
The way kube2iam works is that each node in your cluster will need an IAM policy attached which allows it to assume the roles for your pods.


Create an IAM policy for your nodes and attach it to the the role your Kubernetes nodes run on
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "sts:AssumeRole"
      ],
      "Resource": “*”
    }
  ]
}


 Next create roles for each pod. Each role will need a policy that has only the
 permissions that the pod needs to perform its function e.g. listing s3 objects, writing
 to DynamoDB, reading from SQS, etc. For each role you create, you need to update the assume
 role policy so that your nodes can assume the role. Replace YOUR_NODE_ROLE with
 the arn of the role your Kubernetes nodes run with.


 Assume role policy:

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": "sts:AssumeRole",
      "Principal": {
        "Service": "ec2.amazonaws.com"
      },
      "Effect": "Allow",
      "Sid": ""
    },
    {
      "Sid": "",
      "Effect": "Allow",
      "Principal": {
        "AWS": "YOUR_NODE_ROLE"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}

 Step 2: Add Annotations to Pods:
 The next step is to annotate your pods with the role they should use.
 Just add an annotation in the pod metadata spec, and kube2iam will use that role when authenticating with IAM for the pod.
 Kube2iam will automatically detect the base arn for your role when configured to do so, but you can also specify a full arn
 (beginning with arn:aws:iam) if you need to assume roles in other AWS accounts. The kube2iam documentation has several
 examples of annotating different pod controllers.

 annotations:
   iam.amazonaws.com/role: MY_ROLE_NAME


Step 3: Deploy Kube2iam
Now you are ready to deploy kube2iam. You can reference the kube2iam github
repo to get examples for running in EKS or OpenShift, but I will also go over
the general deployment method here. The first step is to set up RBAC:
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kube2iam
  namespace: kube-system
---
apiVersion: v1
items:
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      name: kube2iam
    rules:
      - apiGroups: [""]
        resources: ["namespaces","pods"]
        verbs: ["get","watch","list"]
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: kube2iam
    subjects:
    - kind: ServiceAccount
      name: kube2iam
      namespace: kube-system
    roleRef:
      kind: ClusterRole
      name: kube2iam
      apiGroup: rbac.authorization.k8s.io
kind: List


Since kube2iam modifies the iptables on your Kubernetes nodes to hijack traffic to the EC2 metadata service, I recommend adding a new node to your cluster that is tainted so you can do a controlled test to make sure everything is set up correctly without affecting your production pods. Add a node to your cluster and then taint it so other pods will not run on it:

kubectl taint nodes NODE_NAME kube2iam=kube2iam:NoSchedule


Step 5: Full Kube2iam Deployment


Remove the nodeName key and kube2iam:kube2iam tolerations from your kube2iam DaemonSet to allow it to run on every node. Once it is installed on each node, you should roll out an update to critical pods to ensure that those pods begin using kube2iam for authentication immediately. Other pods that were using the node role to authenticate will begin going to kube2iam when their temporary credentials expire (usually about an hour). Check your application logs and the kube2iam logs for any IAM errors.

Once everything is running correctly, you can remove the quarantine node that was added earlier.

If you encounter issues, you can delete the agent from all nodes, but it will not automatically clean up the iptables rule it created. This will cause all of the calls to EC2 metadata to go nowhere. You will have to ssh into each node individually and remove the iptable rule yourself.

First list the iptable rules to find the one set up by kube2iam

sudo iptables -t nat -S PREROUTING | grep 169.254.169.254
The output should be similar to

-A PREROUTING -d 169.254.169.254/32 -i weave -p tcp -m tcp --dport 80 -j DNAT --to-destination 10.0.101.101:8181
You may see multiple results if you deployed the agent with different --host-interface options on accident. You can delete them one at a time. To delete a rule, use the -D option of iptables and specify the entire line of output from above after -A. For example:

sudo iptables -t nat -D PREROUTING -d 169.254.169.254/32 -i weave -p tcp -m tcp -
